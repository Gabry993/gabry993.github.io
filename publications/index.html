<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Publications - Gabriele Abbate</title>
  <meta name="description" content="Webpage of Gabriele Abbate">
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="/publications/">
  <link rel="shortcut icon" type ="image/x-icon" href="/favicon.ico">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

  <link rel="preconnect" href="https://player.vimeo.com">
  <link rel="preconnect" href="https://i.vimeocdn.com">
  <link rel="preconnect" href="https://f.vimeocdn.com">



<!-- Google Analytics (original) -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');

</script>

<!-- Global site tag (gtag.js) - Google Analytics 4 -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', '');
</script>

<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
<!-- End Google Tag Manager -->



</head>


  <body>

    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

<nav class="navbar sticky-top navbar-expand-md navbar-dark bg-primary">
    <a class="navbar-brand" href="/">
     <img src="/favicon.ico" width="30" height="30" style="margin-right:5px" class="d-inline-block align-top" alt="">
      Gabriele Abbate
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarColor02">
        <ul class="navbar-nav mr-auto">
        <ul class="navbar-nav">
          <li class="nav-item">
              <a class="nav-link" href="/">Home</a>
          </li> 
          
           <li class="nav-item">
            <a class="nav-link" href="/about">About</a>
           </li> 
          
           <li class="nav-item">
            <a class="nav-link" href="/publications">Publications</a>
           </li> 
          
           <li class="nav-item">
            <a class="nav-link" href="/news">News</a>
           </li> 
          
        </ul>
  </div>
</nav>



    <div class="container-fluid">
      <div class="row">
        <div id="gridid" class="col-sm-12 col-xs-12">
  <style>
.jumbotron{
    padding:3%;
    padding-bottom:10px;
    padding-top:10px;
    margin-top:10px;
    margin-bottom:30px;
}
</style>

<!-- <div class="jumbotron">
### Preprints
<ol class="bibliography" reversed="reversed"></ol>
</div> -->

<div class="jumbotron">
  <h3 id="journal-articles">Journal articles</h3>
  <ol class="bibliography" reversed="reversed"><li><style>
.btn{
/*    margin-bottom:5px;*/
    padding-top:0px;
    padding-bottom:0px;
/*    padding-left:15px;*/
/*    padding-right:15px;*/
    width:40px;
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify">
<!-- <span id="lamberti2024drone">Lamberti, L., Cereda, E., Abbate, G., Bellone, L., Morinigo, V. J. K., Barciś, M., Barciś, A., Giusti, A., Conti, F., &amp; Palossi, D. (2024). <b>A Sim-to-Real Deep Learning-Based Framework for Autonomous Nano-Drone Racing</b>. <i>IEEE Robotics and Automation Letters</i>, <i>9</i>(2), 1899–1906.</span></div> -->
<!-- https://www.w3schools.com/icons/fontawesome_icons_webapp.asp -->
<!-- You can use the below to make your name bold -->
<span id="lamberti2024drone">Lamberti, L., Cereda, E., <b><u>Abbate, G.</u></b>, Bellone, L., Morinigo, V. J. K., Barciś, M., Barciś, A., Giusti, A., Conti, F., &amp; Palossi, D. (2024). <b>A Sim-to-Real Deep Learning-Based Framework for Autonomous Nano-Drone Racing</b>. <i>IEEE Robotics and Automation Letters</i>, <i>9</i>(2), 1899–1906.</span></div>
<!-- <span id="lamberti2024drone">Lamberti, L., Cereda, E., <b><b>Abbate, G.</b></b>, Bellone, L., Morinigo, V. J. K., Barciś, M., Barciś, A., Giusti, A., Conti, F., &amp; Palossi, D. (2024). <b>A Sim-to-Real Deep Learning-Based Framework for Autonomous Nano-Drone Racing</b>. <i>IEEE Robotics and Automation Letters</i>, <i>9</i>(2), 1899–1906.</span></div> -->
<!-- <span id="lamberti2024drone">Lamberti, L., Cereda, E., <p style="font-weight: 100;"><p style="font-weight: 100;">Abbate, G.</p></p>, Bellone, L., Morinigo, V. J. K., Barciś, M., Barciś, A., Giusti, A., Conti, F., &amp; Palossi, D. (2024). <b>A Sim-to-Real Deep Learning-Based Framework for Autonomous Nano-Drone Racing</b>. <i>IEEE Robotics and Automation Letters</i>, <i>9</i>(2), 1899–1906.</span></div> -->





<a href="/papers/lamberti2024drone.pdf" target="_blank"><button class="btn btn-primary"><i class="fa fa-file"></i></button></a>




<a href="http://doi.org/10.1109/LRA.2024.3349814" target="_blank"><button class="btn btn-primary"><i class="ai ai-doi"></i></button></a>




<a href="https://www.youtube.com/watch?v=vHTAwUsj-nk" target="_blank"><button class="btn btn-primary"><i class="fa fa-video-camera"></i></button></a>




<!--  -->




<button class="btn btn-primary" onclick="toggleBibtexlamberti2024drone()"><i class="fa fa-quote-right"></i></button>



<button class="btn btn-primary" onclick="toggleAbstractlamberti2024drone()"><i class="fa fa-info"></i></button>



<div id="alamberti2024drone" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>@article{lamberti2024drone,
  author = {Lamberti, Lorenzo and Cereda, Elia and Abbate, Gabriele and Bellone, Lorenzo and Morinigo, Victor Javier Kartsch and Barciś, Michał and Barciś, Agata and Giusti, Alessandro and Conti, Francesco and Palossi, Daniele},
  journal = {IEEE Robotics and Automation Letters},
  title = {A Sim-to-Real Deep Learning-Based Framework for Autonomous Nano-Drone Racing},
  year = {2024},
  volume = {9},
  number = {2},
  pages = {1899-1906},
  keywords = {},
  doi = {10.1109/LRA.2024.3349814},
  issn = {2377-3766},
  month = feb,
  file = {lamberti2024drone.pdf},
  video = {https://www.youtube.com/watch?v=vHTAwUsj-nk}
}
</pre>
</div>


<div id="blamberti2024drone" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>Autonomous drone racing competitions are a proxy to improve unmanned aerial vehicles’ perception, planning, and control skills. The recent emergence of autonomous nano-sized drone racing imposes new challenges, as their ∼\text10 \,\textc\textm form factor heavily restricts the resources available onboard, including memory, computation, and sensors. This letter describes the methodology and technical implementation of the system winning the first autonomous nano-drone racing international competition: the “IMAV 2022 Nanocopter AI Challenge.” We developed a fully onboard deep learning approach for visual navigation trained only on simulation images to achieve this goal. Our approach includes a convolutional neural network for obstacle avoidance, a sim-to-real dataset collection procedure, and a navigation policy that we selected, characterized, and adapted through simulation and actual in-field experiments. Our system ranked 1\textst among six competing teams at the competition. In our best attempt, we scored \text115 \,\textm of traveled distance in the allotted 5-minute flight, never crashing while dodging static and dynamic obstacles. Sharing our knowledge with the research community, we aim to provide a solid groundwork to foster future development in this field.</pre>
</div>

<script>
function toggleBibtexlamberti2024drone(parameter) {
    var x= document.getElementById('alamberti2024drone');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractlamberti2024drone(parameter) {
    var x= document.getElementById('blamberti2024drone');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
/*    margin-bottom:5px;*/
    padding-top:0px;
    padding-bottom:0px;
/*    padding-left:15px;*/
/*    padding-right:15px;*/
    width:40px;
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify">
<!-- <span id="abbate2023mirror">Abbate, G., Giusti, A., Randazzo, L., &amp; Paolillo, A. (2023). <b>A mirror therapy system using virtual reality and an actuated exoskeleton for the recovery of hand motor impairments: a study of acceptability, usability, and embodiment</b>. <i>Scientific Reports</i>, <i>13</i>, 22881.</span></div> -->
<!-- https://www.w3schools.com/icons/fontawesome_icons_webapp.asp -->
<!-- You can use the below to make your name bold -->
<span id="abbate2023mirror"><b><u>Abbate, G.</u></b>, Giusti, A., Randazzo, L., &amp; Paolillo, A. (2023). <b>A mirror therapy system using virtual reality and an actuated exoskeleton for the recovery of hand motor impairments: a study of acceptability, usability, and embodiment</b>. <i>Scientific Reports</i>, <i>13</i>, 22881.</span></div>
<!-- <span id="abbate2023mirror"><b><b>Abbate, G.</b></b>, Giusti, A., Randazzo, L., &amp; Paolillo, A. (2023). <b>A mirror therapy system using virtual reality and an actuated exoskeleton for the recovery of hand motor impairments: a study of acceptability, usability, and embodiment</b>. <i>Scientific Reports</i>, <i>13</i>, 22881.</span></div> -->
<!-- <span id="abbate2023mirror"><p style="font-weight: 100;"><p style="font-weight: 100;">Abbate, G.</p></p>, Giusti, A., Randazzo, L., &amp; Paolillo, A. (2023). <b>A mirror therapy system using virtual reality and an actuated exoskeleton for the recovery of hand motor impairments: a study of acceptability, usability, and embodiment</b>. <i>Scientific Reports</i>, <i>13</i>, 22881.</span></div> -->





<a href="/papers/abbate2023mirror.pdf" target="_blank"><button class="btn btn-primary"><i class="fa fa-file"></i></button></a>




<a href="http://doi.org/10.1038/s41598-023-49571-7" target="_blank"><button class="btn btn-primary"><i class="ai ai-doi"></i></button></a>




<a href="https://www.youtube.com/watch?v=znBTWJ_E1Bc" target="_blank"><button class="btn btn-primary"><i class="fa fa-video-camera"></i></button></a>




<!--  -->




<button class="btn btn-primary" onclick="toggleBibtexabbate2023mirror()"><i class="fa fa-quote-right"></i></button>



<button class="btn btn-primary" onclick="toggleAbstractabbate2023mirror()"><i class="fa fa-info"></i></button>



<div id="aabbate2023mirror" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>@article{abbate2023mirror,
  author = {Abbate, Gabriele and Giusti, Alessandro and Randazzo, Luca and Paolillo, Antonio},
  year = {2023},
  month = dec,
  pages = {22881},
  title = {A mirror therapy system using virtual reality and an actuated exoskeleton for the recovery of hand motor impairments: a study of acceptability, usability, and embodiment},
  volume = {13},
  journal = {Scientific Reports},
  doi = {10.1038/s41598-023-49571-7},
  file = {abbate2023mirror.pdf},
  video = {https://www.youtube.com/watch?v=znBTWJ_E1Bc}
}
</pre>
</div>


<div id="babbate2023mirror" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>Hand motor impairments are one of the main causes of disabilities worldwide. Rehabilitation procedures like mirror therapy are given crucial importance. In the traditional setup, the patient moves the healthy hand in front of a mirror; the view of the mirrored motion tricks the brain into thinking that the impaired hand is moving as well, stimulating the recovery of the lost hand functionalities. We propose an innovative mirror therapy system that leverages and couples cutting-edge technologies. Virtual reality recreates an immersive and effective mirroring effect; a soft hand exoskeleton accompanies the virtual visual perception by physically inducing the mirrored motion to the real hand. Three working modes of our system have been tested with 21 healthy users. The system is ranked as acceptable by the system usability scale; it does not provoke adverse events or sickness in the users, according to the simulator sickness questionnaire; the three execution modes are also compared w.r.t. the sense of embodiment, evaluated through another customized questionnaire. The achieved results show the potential of our system as a clinical tool and reveal its social and economic impact.</pre>
</div>

<script>
function toggleBibtexabbate2023mirror(parameter) {
    var x= document.getElementById('aabbate2023mirror');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractabbate2023mirror(parameter) {
    var x= document.getElementById('babbate2023mirror');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
/*    margin-bottom:5px;*/
    padding-top:0px;
    padding-bottom:0px;
/*    padding-left:15px;*/
/*    padding-right:15px;*/
    width:40px;
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify">
<!-- <span id="abbate2023self">Abbate, G., Giusti, A., Schmuck, V., Celiktutan, O., &amp; Paolillo, A. (2023). <b>Self-supervised prediction of the intention to interact with a service robot</b>. <i>Robotics and Autonomous Systems</i>, 104568.</span></div> -->
<!-- https://www.w3schools.com/icons/fontawesome_icons_webapp.asp -->
<!-- You can use the below to make your name bold -->
<span id="abbate2023self"><b><u>Abbate, G.</u></b>, Giusti, A., Schmuck, V., Celiktutan, O., &amp; Paolillo, A. (2023). <b>Self-supervised prediction of the intention to interact with a service robot</b>. <i>Robotics and Autonomous Systems</i>, 104568.</span></div>
<!-- <span id="abbate2023self"><b><b>Abbate, G.</b></b>, Giusti, A., Schmuck, V., Celiktutan, O., &amp; Paolillo, A. (2023). <b>Self-supervised prediction of the intention to interact with a service robot</b>. <i>Robotics and Autonomous Systems</i>, 104568.</span></div> -->
<!-- <span id="abbate2023self"><p style="font-weight: 100;"><p style="font-weight: 100;">Abbate, G.</p></p>, Giusti, A., Schmuck, V., Celiktutan, O., &amp; Paolillo, A. (2023). <b>Self-supervised prediction of the intention to interact with a service robot</b>. <i>Robotics and Autonomous Systems</i>, 104568.</span></div> -->





<a href="/papers/abbate2023self.pdf" target="_blank"><button class="btn btn-primary"><i class="fa fa-file"></i></button></a>




<a href="http://doi.org/10.1016/j.robot.2023.104568" target="_blank"><button class="btn btn-primary"><i class="ai ai-doi"></i></button></a>




<a href="https://www.youtube.com/watch?v=mdZDIsr5tcU" target="_blank"><button class="btn btn-primary"><i class="fa fa-video-camera"></i></button></a>




<!--  -->




<button class="btn btn-primary" onclick="toggleBibtexabbate2023self()"><i class="fa fa-quote-right"></i></button>



<button class="btn btn-primary" onclick="toggleAbstractabbate2023self()"><i class="fa fa-info"></i></button>



<div id="aabbate2023self" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>@article{abbate2023self,
  title = {Self-supervised prediction of the intention to interact with a service robot},
  author = {Abbate, Gabriele and Giusti, Alessandro and Schmuck, Viktor and Celiktutan, Oya and Paolillo, Antonio},
  journal = {Robotics and Autonomous Systems},
  pages = {104568},
  year = {2023},
  publisher = {North-Holland},
  doi = {10.1016/j.robot.2023.104568},
  file = {abbate2023self.pdf},
  video = {https://www.youtube.com/watch?v=mdZDIsr5tcU}
}
</pre>
</div>


<div id="babbate2023self" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>A service robot can provide a smoother interaction experience if it has the ability to proactively detect whether a nearby user intends to interact, in order to adapt its behavior e.g. by explicitly showing that it is available to provide a service. In this work, we propose a learning-based approach to predict the probability that a human user will interact with a robot before the interaction actually begins; the approach is self-supervised because after each encounter with a human, the robot can automatically label it depending on whether it resulted in an interaction or not. We explore different classification approaches, using different sets of features considering the pose and the motion of the user. We validate and deploy the approach in three scenarios. The first collects 3442 natural sequences (both interacting and non-interacting) representing employees in an office break area: a real-world, challenging setting, where we consider a coffee machine in place of a service robot. The other two scenarios represent researchers interacting with service robots (200 and 72 sequences, respectively). Results show that, even in challenging real-world settings, our approach can learn without external supervision, and can achieve accurate classification (i.e. AUROC greater than 0.9) of the user’s intention to interact with an advance of more than 3 s before the interaction actually occurs.</pre>
</div>

<script>
function toggleBibtexabbate2023self(parameter) {
    var x= document.getElementById('aabbate2023self');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractabbate2023self(parameter) {
    var x= document.getElementById('babbate2023self');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
/*    margin-bottom:5px;*/
    padding-top:0px;
    padding-bottom:0px;
/*    padding-left:15px;*/
/*    padding-right:15px;*/
    width:40px;
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify">
<!-- <span id="paolillo2022towards">Paolillo, A., Abbate, G., Giusti, A., Trakić, Š., Dzafic, H., Fritz, A., &amp; Guzzi, J. (2022). <b>Towards the integration of a pointing-based human-machine interface in an industrial control system compliant with the iec 61499 standard</b>. <i>Procedia CIRP</i>, <i>107</i>, 1077–1082.</span></div> -->
<!-- https://www.w3schools.com/icons/fontawesome_icons_webapp.asp -->
<!-- You can use the below to make your name bold -->
<span id="paolillo2022towards">Paolillo, A., <b><u>Abbate, G.</u></b>, Giusti, A., Trakić, Š., Dzafic, H., Fritz, A., &amp; Guzzi, J. (2022). <b>Towards the integration of a pointing-based human-machine interface in an industrial control system compliant with the iec 61499 standard</b>. <i>Procedia CIRP</i>, <i>107</i>, 1077–1082.</span></div>
<!-- <span id="paolillo2022towards">Paolillo, A., <b><b>Abbate, G.</b></b>, Giusti, A., Trakić, Š., Dzafic, H., Fritz, A., &amp; Guzzi, J. (2022). <b>Towards the integration of a pointing-based human-machine interface in an industrial control system compliant with the iec 61499 standard</b>. <i>Procedia CIRP</i>, <i>107</i>, 1077–1082.</span></div> -->
<!-- <span id="paolillo2022towards">Paolillo, A., <p style="font-weight: 100;"><p style="font-weight: 100;">Abbate, G.</p></p>, Giusti, A., Trakić, Š., Dzafic, H., Fritz, A., &amp; Guzzi, J. (2022). <b>Towards the integration of a pointing-based human-machine interface in an industrial control system compliant with the iec 61499 standard</b>. <i>Procedia CIRP</i>, <i>107</i>, 1077–1082.</span></div> -->





<a href="/papers/paolillo2022towards.pdf" target="_blank"><button class="btn btn-primary"><i class="fa fa-file"></i></button></a>




<a href="http://doi.org/10.1016/j.procir.2022.05.111" target="_blank"><button class="btn btn-primary"><i class="ai ai-doi"></i></button></a>







<!--  -->




<button class="btn btn-primary" onclick="toggleBibtexpaolillo2022towards()"><i class="fa fa-quote-right"></i></button>



<button class="btn btn-primary" onclick="toggleAbstractpaolillo2022towards()"><i class="fa fa-info"></i></button>



<div id="apaolillo2022towards" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>@article{paolillo2022towards,
  title = {Towards the integration of a pointing-based human-machine interface in an industrial control system compliant with the iec 61499 standard},
  author = {Paolillo, Antonio and Abbate, Gabriele and Giusti, Alessandro and Traki{\'c}, {\v{S}}ejla and Dzafic, Hilmo and Fritz, Artur and Guzzi, J{\'e}r{\^o}me},
  journal = {Procedia CIRP},
  volume = {107},
  pages = {1077--1082},
  year = {2022},
  publisher = {Elsevier},
  doi = {10.1016/j.procir.2022.05.111},
  file = {paolillo2022towards.pdf}
}
</pre>
</div>


<div id="bpaolillo2022towards" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>In the context of Industry 4.0, we tackle the problem of how human operators can interact with a cyber-physical system at run-time. We focus on sporadic interactions where operators, normally occupied with other activities, need to communicate to the system a piece of information relative to a specif part of the plant, such as an anomaly. As a concrete instance of this problem, we consider an automated system composed of a robot loading packages on a conveyor belt that transports and sorts them. We argue that gesture-based interaction modalities offer important advantages for the operators in the considered scenario and can be deployed respecting the IEC 61499 standard.</pre>
</div>

<script>
function toggleBibtexpaolillo2022towards(parameter) {
    var x= document.getElementById('apaolillo2022towards');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractpaolillo2022towards(parameter) {
    var x= document.getElementById('bpaolillo2022towards');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
/*    margin-bottom:5px;*/
    padding-top:0px;
    padding-bottom:0px;
/*    padding-left:15px;*/
/*    padding-right:15px;*/
    width:40px;
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify">
<!-- <span id="can2021semantic">Can, G., Mantegazza, D., Abbate, G., Chappuis, S., &amp; Giusti, A. (2021). <b>Semantic segmentation on Swiss3DCities: A benchmark study on aerial photogrammetric 3D pointcloud dataset</b>. <i>Pattern Recognition Letters</i>, <i>150</i>, 108–114.</span></div> -->
<!-- https://www.w3schools.com/icons/fontawesome_icons_webapp.asp -->
<!-- You can use the below to make your name bold -->
<span id="can2021semantic">Can, G., Mantegazza, D., <b><u>Abbate, G.</u></b>, Chappuis, S., &amp; Giusti, A. (2021). <b>Semantic segmentation on Swiss3DCities: A benchmark study on aerial photogrammetric 3D pointcloud dataset</b>. <i>Pattern Recognition Letters</i>, <i>150</i>, 108–114.</span></div>
<!-- <span id="can2021semantic">Can, G., Mantegazza, D., <b><b>Abbate, G.</b></b>, Chappuis, S., &amp; Giusti, A. (2021). <b>Semantic segmentation on Swiss3DCities: A benchmark study on aerial photogrammetric 3D pointcloud dataset</b>. <i>Pattern Recognition Letters</i>, <i>150</i>, 108–114.</span></div> -->
<!-- <span id="can2021semantic">Can, G., Mantegazza, D., <p style="font-weight: 100;"><p style="font-weight: 100;">Abbate, G.</p></p>, Chappuis, S., &amp; Giusti, A. (2021). <b>Semantic segmentation on Swiss3DCities: A benchmark study on aerial photogrammetric 3D pointcloud dataset</b>. <i>Pattern Recognition Letters</i>, <i>150</i>, 108–114.</span></div> -->





<a href="/papers/can2021semantic.pdf" target="_blank"><button class="btn btn-primary"><i class="fa fa-file"></i></button></a>




<a href="http://doi.org/10.1016/j.patrec.2021.06.004" target="_blank"><button class="btn btn-primary"><i class="ai ai-doi"></i></button></a>






<a href="https://github.com/idsia-robotics/RandLA-Net-pytorch" target="_blank"><button class="btn btn-primary"><i class="fa fa-code"></i></button></a>


<!--  -->




<button class="btn btn-primary" onclick="toggleBibtexcan2021semantic()"><i class="fa fa-quote-right"></i></button>



<button class="btn btn-primary" onclick="toggleAbstractcan2021semantic()"><i class="fa fa-info"></i></button>



<div id="acan2021semantic" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>@article{can2021semantic,
  title = {Semantic segmentation on Swiss3DCities: A benchmark study on aerial photogrammetric 3D pointcloud dataset},
  author = {Can, G{\"u}lcan and Mantegazza, Dario and Abbate, Gabriele and Chappuis, S{\'e}bastien and Giusti, Alessandro},
  journal = {Pattern Recognition Letters},
  volume = {150},
  pages = {108--114},
  year = {2021},
  publisher = {Elsevier},
  doi = {10.1016/j.patrec.2021.06.004},
  file = {can2021semantic.pdf},
  info = {https://github.com/idsia-robotics/RandLA-Net-pytorch}
}
</pre>
</div>


<div id="bcan2021semantic" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>We introduce a new outdoor urban 3D pointcloud dataset, covering a total area of 2.7 km 2, sampled from three Swiss cities with different characteristics. The dataset is manually annotated for semantic segmentation with per-point labels, and is built using photogrammetry from images acquired by multirotors equipped with high-resolution cameras. In contrast to datasets acquired with ground LiDAR sensors, the resulting point clouds are uniformly dense and complete, and are useful to disparate applications, including autonomous driving, gaming and smart city planning. As a benchmark, we report quantitative results of PointNet++, an established point-based deep 3D semantic segmentation model; on this model, we additionally study the impact of using different cities for model generalization.</pre>
</div>

<script>
function toggleBibtexcan2021semantic(parameter) {
    var x= document.getElementById('acan2021semantic');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractcan2021semantic(parameter) {
    var x= document.getElementById('bcan2021semantic');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li></ol>
</div>

<div class="jumbotron">
  <h3 id="conference-proceedings">Conference proceedings</h3>
  <ol class="bibliography" reversed="reversed"><li><style>
.btn{
/*    margin-bottom:5px;*/
    padding-top:0px;
    padding-bottom:0px;
/*    padding-left:15px;*/
/*    padding-right:15px;*/
    width:40px;
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify">
<!-- <span id="arreghini2024iros">Arreghini, S., Abbate, G., Giusti, A., &amp; Paolillo, A. (2024). <b>A Service Robot in the Wild: Analysis of Users Intentions, Robot Behaviors, and Their Impact on the Interaction</b>. <i>2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>, –.</span></div> -->
<!-- https://www.w3schools.com/icons/fontawesome_icons_webapp.asp -->
<!-- You can use the below to make your name bold -->
<span id="arreghini2024iros">Arreghini, S., <b><u>Abbate, G.</u></b>, Giusti, A., &amp; Paolillo, A. (2024). <b>A Service Robot in the Wild: Analysis of Users Intentions, Robot Behaviors, and Their Impact on the Interaction</b>. <i>2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>, –.</span></div>
<!-- <span id="arreghini2024iros">Arreghini, S., <b><b>Abbate, G.</b></b>, Giusti, A., &amp; Paolillo, A. (2024). <b>A Service Robot in the Wild: Analysis of Users Intentions, Robot Behaviors, and Their Impact on the Interaction</b>. <i>2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>, –.</span></div> -->
<!-- <span id="arreghini2024iros">Arreghini, S., <p style="font-weight: 100;"><p style="font-weight: 100;">Abbate, G.</p></p>, Giusti, A., &amp; Paolillo, A. (2024). <b>A Service Robot in the Wild: Analysis of Users Intentions, Robot Behaviors, and Their Impact on the Interaction</b>. <i>2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</i>, –.</span></div> -->





<a href="/papers/arreghini2024iros.pdf" target="_blank"><button class="btn btn-primary"><i class="fa fa-file"></i></button></a>





<a href="https://www.youtube.com/watch?v=NNgbNRxm5V4" target="_blank"><button class="btn btn-primary"><i class="fa fa-video-camera"></i></button></a>



<a href="https://github.com/idsia-robotics/service-robot-in-the-wild" target="_blank"><button class="btn btn-primary"><i class="fa fa-code"></i></button></a>


<!--  -->




<button class="btn btn-primary" onclick="toggleBibtexarreghini2024iros()"><i class="fa fa-quote-right"></i></button>



<button class="btn btn-primary" onclick="toggleAbstractarreghini2024iros()"><i class="fa fa-info"></i></button>



<div id="aarreghini2024iros" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>@inproceedings{arreghini2024iros,
  author = {Arreghini, S. and Abbate, G. and Giusti, A. and Paolillo, A.},
  title = {A Service Robot in the Wild: Analysis of Users Intentions, Robot Behaviors, and Their Impact on the Interaction},
  booktitle = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages = {--},
  year = {2024},
  month = oct,
  file = {arreghini2024iros.pdf},
  video = {https://www.youtube.com/watch?v=NNgbNRxm5V4},
  info = {https://github.com/idsia-robotics/service-robot-in-the-wild}
}
</pre>
</div>


<div id="barreghini2024iros" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>We consider a service robot that offers chocolate treats to people passing in its proximity: it has the capability of predicting in advance a person’s intention to interact, and to actuate an "offering" gesture, subtly extending the tray of chocolates towards a given target. We run the system for more than 5 hours across 3 days and two different crowded public locations; the system implements three possible behaviors that are randomly toggled every few minutes: passive (e.g. never performing the offering gesture); or active, triggered by either a naive distance-based rule, or a smart approach that relies on various behavioral cues of the user. We collect a real-world dataset that includes information on 1777 users with several spontaneous human-robot interactions and study the influence of robot actions on people’s behavior. Our comprehensive analysis suggests that users are more prone to engage with the robot when it proactively starts the interaction. We release the dataset and provide insights to make our work reproducible for the community. Also, we report qualitative observations collected during the acquisition campaign and identify future challenges and research directions in the domain of social human-robot interaction.</pre>
</div>

<script>
function toggleBibtexarreghini2024iros(parameter) {
    var x= document.getElementById('aarreghini2024iros');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractarreghini2024iros(parameter) {
    var x= document.getElementById('barreghini2024iros');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
/*    margin-bottom:5px;*/
    padding-top:0px;
    padding-bottom:0px;
/*    padding-left:15px;*/
/*    padding-right:15px;*/
    width:40px;
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify">
<!-- <span id="arreghini2024icra">Arreghini, S., Abbate, G., Giusti, A., &amp; Paolillo, A. (2024). <b>Predicting the Intention to Interact with a Service Robot: the Role of Gaze Cues</b>. <i>2024 IEEE International Conference on Robotics and Automation (ICRA)</i>, 993–999.</span></div> -->
<!-- https://www.w3schools.com/icons/fontawesome_icons_webapp.asp -->
<!-- You can use the below to make your name bold -->
<span id="arreghini2024icra">Arreghini, S., <b><u>Abbate, G.</u></b>, Giusti, A., &amp; Paolillo, A. (2024). <b>Predicting the Intention to Interact with a Service Robot: the Role of Gaze Cues</b>. <i>2024 IEEE International Conference on Robotics and Automation (ICRA)</i>, 993–999.</span></div>
<!-- <span id="arreghini2024icra">Arreghini, S., <b><b>Abbate, G.</b></b>, Giusti, A., &amp; Paolillo, A. (2024). <b>Predicting the Intention to Interact with a Service Robot: the Role of Gaze Cues</b>. <i>2024 IEEE International Conference on Robotics and Automation (ICRA)</i>, 993–999.</span></div> -->
<!-- <span id="arreghini2024icra">Arreghini, S., <p style="font-weight: 100;"><p style="font-weight: 100;">Abbate, G.</p></p>, Giusti, A., &amp; Paolillo, A. (2024). <b>Predicting the Intention to Interact with a Service Robot: the Role of Gaze Cues</b>. <i>2024 IEEE International Conference on Robotics and Automation (ICRA)</i>, 993–999.</span></div> -->





<a href="/papers/arreghini2024icra.pdf" target="_blank"><button class="btn btn-primary"><i class="fa fa-file"></i></button></a>





<a href="https://youtu.be/5V6efrEtfBI" target="_blank"><button class="btn btn-primary"><i class="fa fa-video-camera"></i></button></a>



<a href="https://github.com/idsia-robotics/intention-to-interact-detector-gaze" target="_blank"><button class="btn btn-primary"><i class="fa fa-code"></i></button></a>


<!--  -->




<button class="btn btn-primary" onclick="toggleBibtexarreghini2024icra()"><i class="fa fa-quote-right"></i></button>



<button class="btn btn-primary" onclick="toggleAbstractarreghini2024icra()"><i class="fa fa-info"></i></button>



<div id="aarreghini2024icra" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>@inproceedings{arreghini2024icra,
  author = {Arreghini, Simone and Abbate, Gabriele and Giusti, Alessandro and Paolillo, Antonio},
  booktitle = {2024 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {Predicting the Intention to Interact with a Service Robot: the Role of Gaze Cues},
  year = {2024},
  pages = {993-999},
  keywords = {Accuracy;Service robots;User experience;Task analysis;Testing},
  month = may,
  file = {arreghini2024icra.pdf},
  video = {https://youtu.be/5V6efrEtfBI},
  info = {https://github.com/idsia-robotics/intention-to-interact-detector-gaze}
}
</pre>
</div>


<div id="barreghini2024icra" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>For a service robot, it is crucial to perceive as early as possible that an approaching person intends to interact: in this case, it can proactively enact friendly behaviors that lead to an improved user experience. We solve this perception task with a sequence-to-sequence classifier of a potential user intention to interact, which can be trained in a self-supervised way. Our main contribution is a study of the benefit of features representing the person’s gaze in this context. Extensive experiments on a novel dataset show that the inclusion of gaze cues significantly improves the classifier performance (AUROC increases from 84.5 % to 91.2 %); the distance at which an accurate classification can be achieved improves from 2.4 m to 3.2 m. We also quantify the system’s ability to adapt to new environments without external supervision. Qualitative experiments show practical applications with a waiter robot.</pre>
</div>

<script>
function toggleBibtexarreghini2024icra(parameter) {
    var x= document.getElementById('aarreghini2024icra');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractarreghini2024icra(parameter) {
    var x= document.getElementById('barreghini2024icra');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
/*    margin-bottom:5px;*/
    padding-top:0px;
    padding-bottom:0px;
/*    padding-left:15px;*/
/*    padding-right:15px;*/
    width:40px;
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify">
<!-- <span id="arreghini2024hri">Arreghini, S., Abbate, G., Giusti, A., &amp; Paolillo, A. (2024). <b>A Long-Range Mutual Gaze Detector for HRI</b>. <i>Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction</i>, 870–874.</span></div> -->
<!-- https://www.w3schools.com/icons/fontawesome_icons_webapp.asp -->
<!-- You can use the below to make your name bold -->
<span id="arreghini2024hri">Arreghini, S., <b><u>Abbate, G.</u></b>, Giusti, A., &amp; Paolillo, A. (2024). <b>A Long-Range Mutual Gaze Detector for HRI</b>. <i>Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction</i>, 870–874.</span></div>
<!-- <span id="arreghini2024hri">Arreghini, S., <b><b>Abbate, G.</b></b>, Giusti, A., &amp; Paolillo, A. (2024). <b>A Long-Range Mutual Gaze Detector for HRI</b>. <i>Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction</i>, 870–874.</span></div> -->
<!-- <span id="arreghini2024hri">Arreghini, S., <p style="font-weight: 100;"><p style="font-weight: 100;">Abbate, G.</p></p>, Giusti, A., &amp; Paolillo, A. (2024). <b>A Long-Range Mutual Gaze Detector for HRI</b>. <i>Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction</i>, 870–874.</span></div> -->





<a href="/papers/arreghini2024hri.pdf" target="_blank"><button class="btn btn-primary"><i class="fa fa-file"></i></button></a>





<a href="https://raw.githubusercontent.com/idsia-robotics/mutual_gaze_detector/hri/assets/readme.mp4" target="_blank"><button class="btn btn-primary"><i class="fa fa-video-camera"></i></button></a>



<a href="https://github.com/idsia-robotics/mutual_gaze_detector" target="_blank"><button class="btn btn-primary"><i class="fa fa-code"></i></button></a>


<!--  -->




<button class="btn btn-primary" onclick="toggleBibtexarreghini2024hri()"><i class="fa fa-quote-right"></i></button>



<button class="btn btn-primary" onclick="toggleAbstractarreghini2024hri()"><i class="fa fa-info"></i></button>



<div id="aarreghini2024hri" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>@inproceedings{arreghini2024hri,
  author = {Arreghini, Simone and Abbate, Gabriele and Giusti, Alessandro and Paolillo, Antonio},
  title = {A Long-Range Mutual Gaze Detector for HRI},
  year = {2024},
  month = mar,
  isbn = {9798400703225},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  booktitle = {Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
  pages = {870–874},
  numpages = {5},
  keywords = {human-centered perception, service robots, social robotics},
  location = {Boulder, CO, USA},
  series = {HRI '24},
  file = {arreghini2024hri.pdf},
  video = {https://raw.githubusercontent.com/idsia-robotics/mutual_gaze_detector/hri/assets/readme.mp4},
  info = {https://github.com/idsia-robotics/mutual_gaze_detector}
}
</pre>
</div>


<div id="barreghini2024hri" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>The detection of mutual gaze in the context of human-robot interaction is crucial for the understanding of human partners’ behavior. Indeed, the monitoring of the users’ gaze from a long distance enables the prediction of their intention and allows the robot to be proactive. Nonetheless, current implementations struggle or cannot operate in scenarios where detection from long distances is required. In this work, we propose a ROS2 software pipeline that detects mutual gaze up to 5 m of distance. The code relies on robust off-the-shelf perception algorithms.</pre>
</div>

<script>
function toggleBibtexarreghini2024hri(parameter) {
    var x= document.getElementById('aarreghini2024hri');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractarreghini2024hri(parameter) {
    var x= document.getElementById('barreghini2024hri');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
/*    margin-bottom:5px;*/
    padding-top:0px;
    padding-bottom:0px;
/*    padding-left:15px;*/
/*    padding-right:15px;*/
    width:40px;
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify">
<!-- <span id="abbate2022pointit">Abbate, G., Giusti, A., Paolillo, A., Gromov, B., Gambardella, L., Rizzoli, A. E., &amp; Guzzi, J. (2022). <b>PointIt: A ROS toolkit for interacting with co-located robots using pointing gestures</b>. <i>2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</i>, 608–612.</span></div> -->
<!-- https://www.w3schools.com/icons/fontawesome_icons_webapp.asp -->
<!-- You can use the below to make your name bold -->
<span id="abbate2022pointit"><b><u>Abbate, G.</u></b>, Giusti, A., Paolillo, A., Gromov, B., Gambardella, L., Rizzoli, A. E., &amp; Guzzi, J. (2022). <b>PointIt: A ROS toolkit for interacting with co-located robots using pointing gestures</b>. <i>2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</i>, 608–612.</span></div>
<!-- <span id="abbate2022pointit"><b><b>Abbate, G.</b></b>, Giusti, A., Paolillo, A., Gromov, B., Gambardella, L., Rizzoli, A. E., &amp; Guzzi, J. (2022). <b>PointIt: A ROS toolkit for interacting with co-located robots using pointing gestures</b>. <i>2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</i>, 608–612.</span></div> -->
<!-- <span id="abbate2022pointit"><p style="font-weight: 100;"><p style="font-weight: 100;">Abbate, G.</p></p>, Giusti, A., Paolillo, A., Gromov, B., Gambardella, L., Rizzoli, A. E., &amp; Guzzi, J. (2022). <b>PointIt: A ROS toolkit for interacting with co-located robots using pointing gestures</b>. <i>2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</i>, 608–612.</span></div> -->





<a href="/papers/abbate2022pointit.pdf" target="_blank"><button class="btn btn-primary"><i class="fa fa-file"></i></button></a>




<a href="http://doi.org/10.1109/HRI53351.2022.9889486" target="_blank"><button class="btn btn-primary"><i class="ai ai-doi"></i></button></a>






<a href="https://github.com/Gabry993/pointing-user-interface-hri" target="_blank"><button class="btn btn-primary"><i class="fa fa-code"></i></button></a>


<!--  -->




<button class="btn btn-primary" onclick="toggleBibtexabbate2022pointit()"><i class="fa fa-quote-right"></i></button>



<button class="btn btn-primary" onclick="toggleAbstractabbate2022pointit()"><i class="fa fa-info"></i></button>



<div id="aabbate2022pointit" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>@inproceedings{abbate2022pointit,
  title = {PointIt: A ROS toolkit for interacting with co-located robots using pointing gestures},
  author = {Abbate, Gabriele and Giusti, Alessandro and Paolillo, Antonio and Gromov, Boris and Gambardella, Luca and Rizzoli, Andrea Emilio and Guzzi, J{\'e}r{\^o}me},
  booktitle = {2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
  pages = {608--612},
  year = {2022},
  organization = {IEEE},
  doi = {10.1109/HRI53351.2022.9889486},
  file = {abbate2022pointit.pdf},
  info = {https://github.com/Gabry993/pointing-user-interface-hri}
}
</pre>
</div>


<div id="babbate2022pointit" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>We introduce PointIt, a toolkit for the Robot Operating System (ROS2) to build human-robot interfaces based on pointing gestures sensed by a wrist-worn Inertial Measurement Unit, such as a smartwatch. We release the software as open-source with MIT license; docker images and exhaustive instructions simplify its usage in simulated and real-world deployments.</pre>
</div>

<script>
function toggleBibtexabbate2022pointit(parameter) {
    var x= document.getElementById('aabbate2022pointit');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractabbate2022pointit(parameter) {
    var x= document.getElementById('babbate2022pointit');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
/*    margin-bottom:5px;*/
    padding-top:0px;
    padding-bottom:0px;
/*    padding-left:15px;*/
/*    padding-right:15px;*/
    width:40px;
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify">
<!-- <span id="guzzi2022interacting">Guzzi, J., Abbate, G., Paolillo, A., &amp; Giusti, A. (2022). <b>Interacting with a conveyor belt in virtual reality using pointing gestures</b>. <i>2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</i>, 1194–1195.</span></div> -->
<!-- https://www.w3schools.com/icons/fontawesome_icons_webapp.asp -->
<!-- You can use the below to make your name bold -->
<span id="guzzi2022interacting">Guzzi, J., <b><u>Abbate, G.</u></b>, Paolillo, A., &amp; Giusti, A. (2022). <b>Interacting with a conveyor belt in virtual reality using pointing gestures</b>. <i>2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</i>, 1194–1195.</span></div>
<!-- <span id="guzzi2022interacting">Guzzi, J., <b><b>Abbate, G.</b></b>, Paolillo, A., &amp; Giusti, A. (2022). <b>Interacting with a conveyor belt in virtual reality using pointing gestures</b>. <i>2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</i>, 1194–1195.</span></div> -->
<!-- <span id="guzzi2022interacting">Guzzi, J., <p style="font-weight: 100;"><p style="font-weight: 100;">Abbate, G.</p></p>, Paolillo, A., &amp; Giusti, A. (2022). <b>Interacting with a conveyor belt in virtual reality using pointing gestures</b>. <i>2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</i>, 1194–1195.</span></div> -->





<a href="/papers/guzzi2022interacting.pdf" target="_blank"><button class="btn btn-primary"><i class="fa fa-file"></i></button></a>




<a href="http://doi.org/10.1109/HRI53351.2022.9889380" target="_blank"><button class="btn btn-primary"><i class="ai ai-doi"></i></button></a>




<a href="https://idsia-robotics.github.io/pointing/files/videos/guzzi2022demo.mp4" target="_blank"><button class="btn btn-primary"><i class="fa fa-video-camera"></i></button></a>




<!--  -->




<button class="btn btn-primary" onclick="toggleBibtexguzzi2022interacting()"><i class="fa fa-quote-right"></i></button>



<button class="btn btn-primary" onclick="toggleAbstractguzzi2022interacting()"><i class="fa fa-info"></i></button>



<div id="aguzzi2022interacting" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>@inproceedings{guzzi2022interacting,
  title = {Interacting with a conveyor belt in virtual reality using pointing gestures},
  author = {Guzzi, J{\'e}r{\^o}me and Abbate, Gabriele and Paolillo, Antonio and Giusti, Alessandro},
  booktitle = {2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
  pages = {1194--1195},
  year = {2022},
  organization = {IEEE},
  doi = {10.1109/HRI53351.2022.9889380},
  file = {guzzi2022interacting.pdf},
  video = {https://idsia-robotics.github.io/pointing/files/videos/guzzi2022demo.mp4}
}
</pre>
</div>


<div id="bguzzi2022interacting" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>We present an interactive demonstration where users are immersed in a virtual reality simulation of a logistic automation system. Using pointing gestures sensed by wrist-worn inertial measurement unit, users select defective packages transported on conveyor belts. The demonstration allows users to experience a novel way to interact with automation systems, and shows an effective application of virtual reality for human-robot interaction studies.</pre>
</div>

<script>
function toggleBibtexguzzi2022interacting(parameter) {
    var x= document.getElementById('aguzzi2022interacting');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractguzzi2022interacting(parameter) {
    var x= document.getElementById('bguzzi2022interacting');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
/*    margin-bottom:5px;*/
    padding-top:0px;
    padding-bottom:0px;
/*    padding-left:15px;*/
/*    padding-right:15px;*/
    width:40px;
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify">
<!-- <span id="abbate2022selecting">Abbate, G., Giusti, A., Paolillo, A., Gambardella, L. M., Rizzoli, A. E., &amp; Guzzi, J. (2022). <b>Selecting Objects on Conveyor Belts Using Pointing Gestures Sensed by a Wrist-worn Inertial Measurement Unit</b>. <i>2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)</i>, 633–640.</span></div> -->
<!-- https://www.w3schools.com/icons/fontawesome_icons_webapp.asp -->
<!-- You can use the below to make your name bold -->
<span id="abbate2022selecting"><b><u>Abbate, G.</u></b>, Giusti, A., Paolillo, A., Gambardella, L. M., Rizzoli, A. E., &amp; Guzzi, J. (2022). <b>Selecting Objects on Conveyor Belts Using Pointing Gestures Sensed by a Wrist-worn Inertial Measurement Unit</b>. <i>2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)</i>, 633–640.</span></div>
<!-- <span id="abbate2022selecting"><b><b>Abbate, G.</b></b>, Giusti, A., Paolillo, A., Gambardella, L. M., Rizzoli, A. E., &amp; Guzzi, J. (2022). <b>Selecting Objects on Conveyor Belts Using Pointing Gestures Sensed by a Wrist-worn Inertial Measurement Unit</b>. <i>2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)</i>, 633–640.</span></div> -->
<!-- <span id="abbate2022selecting"><p style="font-weight: 100;"><p style="font-weight: 100;">Abbate, G.</p></p>, Giusti, A., Paolillo, A., Gambardella, L. M., Rizzoli, A. E., &amp; Guzzi, J. (2022). <b>Selecting Objects on Conveyor Belts Using Pointing Gestures Sensed by a Wrist-worn Inertial Measurement Unit</b>. <i>2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)</i>, 633–640.</span></div> -->





<a href="/papers/abbate2022selecting.pdf" target="_blank"><button class="btn btn-primary"><i class="fa fa-file"></i></button></a>




<a href="http://doi.org/10.1109/CASE49997.2022.9926448" target="_blank"><button class="btn btn-primary"><i class="ai ai-doi"></i></button></a>




<a href="https://www.youtube.com/watch?v=fEnPEXPlVIM" target="_blank"><button class="btn btn-primary"><i class="fa fa-video-camera"></i></button></a>




<!--  -->




<button class="btn btn-primary" onclick="toggleBibtexabbate2022selecting()"><i class="fa fa-quote-right"></i></button>



<button class="btn btn-primary" onclick="toggleAbstractabbate2022selecting()"><i class="fa fa-info"></i></button>



<div id="aabbate2022selecting" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>@inproceedings{abbate2022selecting,
  title = {Selecting Objects on Conveyor Belts Using Pointing Gestures Sensed by a Wrist-worn Inertial Measurement Unit},
  author = {Abbate, Gabriele and Giusti, Alessandro and Paolillo, Antonio and Gambardella, Luca Maria and Rizzoli, Andrea Emilio and Guzzi, J{\'e}r{\^o}me},
  booktitle = {2022 IEEE 18th International Conference on Automation Science and Engineering (CASE)},
  pages = {633--640},
  year = {2022},
  organization = {IEEE},
  doi = {10.1109/CASE49997.2022.9926448},
  file = {abbate2022selecting.pdf},
  video = {https://www.youtube.com/watch?v=fEnPEXPlVIM}
}
</pre>
</div>


<div id="babbate2022selecting" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>We introduce an intuitive pointing-based interface to select objects moving on a system of conveyor belts. The interface has minimal sensing requirements, as the operator only needs to wear an Inertial Measurement Unit on the wrist (e.g., a smartwatch). LED strips provide the required visual feedback to precisely point to the objects and select them. We test the proposed approach in three environments of different complexity. Experiments compare our approach with a graphical interface where the user clicks on packages with a mouse; quantitative results show that our interface compares favorably, especially in difficult scenarios involving many packages moving fast.</pre>
</div>

<script>
function toggleBibtexabbate2022selecting(parameter) {
    var x= document.getElementById('aabbate2022selecting');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractabbate2022selecting(parameter) {
    var x= document.getElementById('babbate2022selecting');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
/*    margin-bottom:5px;*/
    padding-top:0px;
    padding-bottom:0px;
/*    padding-left:15px;*/
/*    padding-right:15px;*/
    width:40px;
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify">
<!-- <span id="abbate2021pointing">Abbate, G., Gromov, B., Gambardella, L. M., &amp; Giusti, A. (2021). <b>Pointing at moving robots: Detecting events from wrist IMU data</b>. <i>2021 IEEE International Conference on Robotics and Automation (ICRA)</i>, 3604–3611.</span></div> -->
<!-- https://www.w3schools.com/icons/fontawesome_icons_webapp.asp -->
<!-- You can use the below to make your name bold -->
<span id="abbate2021pointing"><b><u>Abbate, G.</u></b>, Gromov, B., Gambardella, L. M., &amp; Giusti, A. (2021). <b>Pointing at moving robots: Detecting events from wrist IMU data</b>. <i>2021 IEEE International Conference on Robotics and Automation (ICRA)</i>, 3604–3611.</span></div>
<!-- <span id="abbate2021pointing"><b><b>Abbate, G.</b></b>, Gromov, B., Gambardella, L. M., &amp; Giusti, A. (2021). <b>Pointing at moving robots: Detecting events from wrist IMU data</b>. <i>2021 IEEE International Conference on Robotics and Automation (ICRA)</i>, 3604–3611.</span></div> -->
<!-- <span id="abbate2021pointing"><p style="font-weight: 100;"><p style="font-weight: 100;">Abbate, G.</p></p>, Gromov, B., Gambardella, L. M., &amp; Giusti, A. (2021). <b>Pointing at moving robots: Detecting events from wrist IMU data</b>. <i>2021 IEEE International Conference on Robotics and Automation (ICRA)</i>, 3604–3611.</span></div> -->





<a href="/papers/abbate2021pointing.pdf" target="_blank"><button class="btn btn-primary"><i class="fa fa-file"></i></button></a>




<a href="http://doi.org/10.1109/ICRA.2019.8794399" target="_blank"><button class="btn btn-primary"><i class="ai ai-doi"></i></button></a>




<a href="https://www.youtube.com/embed/-Eroidxx9sY?si=MuZBG5ogLqvOBzMR" target="_blank"><button class="btn btn-primary"><i class="fa fa-video-camera"></i></button></a>




<!--  -->




<button class="btn btn-primary" onclick="toggleBibtexabbate2021pointing()"><i class="fa fa-quote-right"></i></button>



<button class="btn btn-primary" onclick="toggleAbstractabbate2021pointing()"><i class="fa fa-info"></i></button>



<div id="aabbate2021pointing" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>@inproceedings{abbate2021pointing,
  title = {Pointing at moving robots: Detecting events from wrist IMU data},
  author = {Abbate, Gabriele and Gromov, Boris and Gambardella, Luca M and Giusti, Alessandro},
  booktitle = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
  pages = {3604--3611},
  year = {2021},
  organization = {IEEE},
  doi = {10.1109/ICRA.2019.8794399},
  file = {abbate2021pointing.pdf},
  video = {https://www.youtube.com/embed/-Eroidxx9sY?si=MuZBG5ogLqvOBzMR}
}
</pre>
</div>


<div id="babbate2021pointing" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>We present a system for interaction between co-located humans and mobile robots, which uses pointing gestures sensed by a wrist-mounted IMU. The operator begins by pointing, for a short time, at a moving robot. The system thus simultaneously determines: that the operator wants to interact; the robot they want to interact with; and the relative pose among the two. Then, the system can reconstruct pointed locations in the robot’s own reference frame, and provide real-time feedback about them so that the user can adapt to misalignments. We discuss the challenges to be solved to implement such a system and propose practical solutions, including variants for fast flying robots and slow ground robots. We report different experiments with real robots and untrained users, validating the individual components and the system as a whole.</pre>
</div>

<script>
function toggleBibtexabbate2021pointing(parameter) {
    var x= document.getElementById('aabbate2021pointing');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractabbate2021pointing(parameter) {
    var x= document.getElementById('babbate2021pointing');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
/*    margin-bottom:5px;*/
    padding-top:0px;
    padding-bottom:0px;
/*    padding-left:15px;*/
/*    padding-right:15px;*/
    width:40px;
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify">
<!-- <span id="gromov2019video">Gromov, B., Guzzi, J., Abbate, G., Gambardella, L., &amp; Giusti, A. (2019). <b>Video: Pointing gestures for proximity interaction</b>. <i>2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</i>, 370–370.</span></div> -->
<!-- https://www.w3schools.com/icons/fontawesome_icons_webapp.asp -->
<!-- You can use the below to make your name bold -->
<span id="gromov2019video">Gromov, B., Guzzi, J., <b><u>Abbate, G.</u></b>, Gambardella, L., &amp; Giusti, A. (2019). <b>Video: Pointing gestures for proximity interaction</b>. <i>2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</i>, 370–370.</span></div>
<!-- <span id="gromov2019video">Gromov, B., Guzzi, J., <b><b>Abbate, G.</b></b>, Gambardella, L., &amp; Giusti, A. (2019). <b>Video: Pointing gestures for proximity interaction</b>. <i>2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</i>, 370–370.</span></div> -->
<!-- <span id="gromov2019video">Gromov, B., Guzzi, J., <p style="font-weight: 100;"><p style="font-weight: 100;">Abbate, G.</p></p>, Gambardella, L., &amp; Giusti, A. (2019). <b>Video: Pointing gestures for proximity interaction</b>. <i>2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)</i>, 370–370.</span></div> -->





<a href="/papers/gromov2019video.pdf" target="_blank"><button class="btn btn-primary"><i class="fa fa-file"></i></button></a>




<a href="http://doi.org/10.1109/HRI.2019.8673020" target="_blank"><button class="btn btn-primary"><i class="ai ai-doi"></i></button></a>




<a href="https://www.youtube.com/watch?v=yafy-HZMk_U" target="_blank"><button class="btn btn-primary"><i class="fa fa-video-camera"></i></button></a>




<!--  -->


<a href="/images/hri2019_award.jpg" target="_blank"><button class="btn btn-primary"><i class="fa fa-trophy"></i></button></a>



<button class="btn btn-primary" onclick="toggleBibtexgromov2019video()"><i class="fa fa-quote-right"></i></button>



<button class="btn btn-primary" onclick="toggleAbstractgromov2019video()"><i class="fa fa-info"></i></button>



<div id="agromov2019video" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>@inproceedings{gromov2019video,
  title = {Video: Pointing gestures for proximity interaction},
  author = {Gromov, Boris and Guzzi, J{\'e}r{\^o}me and Abbate, Gabriele and Gambardella, Luca and Giusti, Alessandro},
  booktitle = {2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
  pages = {370--370},
  year = {2019},
  organization = {IEEE},
  doi = {10.1109/HRI.2019.8673020},
  file = {gromov2019video.pdf},
  video = {https://www.youtube.com/watch?v=yafy-HZMk_U},
  award = {hri2019_award.jpg}
}
</pre>
</div>


<div id="bgromov2019video" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>We propose a system to control robots in the users proximity with pointing gestures-a natural device that people use all the time to communicate with each other. Our system has two requirements: first, the robot must be able to reconstruct its own motion, e.g. by means of visual odometry; second, the user must wear a wristband or smartwatch with an inertial measurement unit. Crucially, the robot does not need to perceive the user in any way. The resulting system is widely applicable, robust, and intuitive to use.</pre>
</div>

<script>
function toggleBibtexgromov2019video(parameter) {
    var x= document.getElementById('agromov2019video');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractgromov2019video(parameter) {
    var x= document.getElementById('bgromov2019video');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
/*    margin-bottom:5px;*/
    padding-top:0px;
    padding-bottom:0px;
/*    padding-left:15px;*/
/*    padding-right:15px;*/
    width:40px;
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify">
<!-- <span id="gromov2019proximity">Gromov, B., Abbate, G., Gambardella, L. M., &amp; Giusti, A. (2019). <b>Proximity human-robot interaction using pointing gestures and a wrist-mounted IMU</b>. <i>2019 International Conference on Robotics and Automation (ICRA)</i>, 8084–8091.</span></div> -->
<!-- https://www.w3schools.com/icons/fontawesome_icons_webapp.asp -->
<!-- You can use the below to make your name bold -->
<span id="gromov2019proximity">Gromov, B., <b><u>Abbate, G.</u></b>, Gambardella, L. M., &amp; Giusti, A. (2019). <b>Proximity human-robot interaction using pointing gestures and a wrist-mounted IMU</b>. <i>2019 International Conference on Robotics and Automation (ICRA)</i>, 8084–8091.</span></div>
<!-- <span id="gromov2019proximity">Gromov, B., <b><b>Abbate, G.</b></b>, Gambardella, L. M., &amp; Giusti, A. (2019). <b>Proximity human-robot interaction using pointing gestures and a wrist-mounted IMU</b>. <i>2019 International Conference on Robotics and Automation (ICRA)</i>, 8084–8091.</span></div> -->
<!-- <span id="gromov2019proximity">Gromov, B., <p style="font-weight: 100;"><p style="font-weight: 100;">Abbate, G.</p></p>, Gambardella, L. M., &amp; Giusti, A. (2019). <b>Proximity human-robot interaction using pointing gestures and a wrist-mounted IMU</b>. <i>2019 International Conference on Robotics and Automation (ICRA)</i>, 8084–8091.</span></div> -->





<a href="/papers/gromov2019proximity.pdf" target="_blank"><button class="btn btn-primary"><i class="fa fa-file"></i></button></a>




<a href="http://doi.org/10.1109/ICRA.2019.8794399" target="_blank"><button class="btn btn-primary"><i class="ai ai-doi"></i></button></a>







<!--  -->




<button class="btn btn-primary" onclick="toggleBibtexgromov2019proximity()"><i class="fa fa-quote-right"></i></button>



<button class="btn btn-primary" onclick="toggleAbstractgromov2019proximity()"><i class="fa fa-info"></i></button>



<div id="agromov2019proximity" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>@inproceedings{gromov2019proximity,
  title = {Proximity human-robot interaction using pointing gestures and a wrist-mounted IMU},
  author = {Gromov, Boris and Abbate, Gabriele and Gambardella, Luca M and Giusti, Alessandro},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  pages = {8084--8091},
  year = {2019},
  organization = {IEEE},
  doi = {10.1109/ICRA.2019.8794399},
  file = {gromov2019proximity.pdf}
}
</pre>
</div>


<div id="bgromov2019proximity" style="display: none; background-color:whitesmoke; border-radius:5px; padding:5px; margin:0px;">
<pre>We present a system for interaction between co-located humans and mobile robots, which uses pointing gestures sensed by a wrist-mounted IMU. The operator begins by pointing, for a short time, at a moving robot. The system thus simultaneously determines: that the operator wants to interact; the robot they want to interact with; and the relative pose among the two. Then, the system can reconstruct pointed locations in the robot’s own reference frame, and provide real-time feedback about them so that the user can adapt to misalignments. We discuss the challenges to be solved to implement such a system and propose practical solutions, including variants for fast flying robots and slow ground robots. We report different experiments with real robots and untrained users, validating the individual components and the system as a whole.</pre>
</div>

<script>
function toggleBibtexgromov2019proximity(parameter) {
    var x= document.getElementById('agromov2019proximity');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractgromov2019proximity(parameter) {
    var x= document.getElementById('bgromov2019proximity');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li></ol>
</div>

</div>

      </div>
    </div>

    <br/>
<section id="footer">
<div class="container-footer">
  <div class="panel-footer">
	  <div class="row">
		<div class="col-sm-4">
		    <h5>About</h5>	
            <p>Gabriele Abbate<br/> Research/Software Engineer<br/> Istituto Dalle Molle di Studi sull'Intelligenza Artificiale (IDSIA) </br>USI-SUPSI
</p>
		</div>

		<div class="col-sm-4">
		    <h5>Contact</h5>	
            <p><a href="#" target="_blank" data-gen-agar><i class="fa fa-envelope fa-1x"></i> Contact Gabriele via email</a> <br/>
</p>
		</div>

		<div class="col-sm-4">
		    <h5>Coordinates</h5>	
            <p>University Campus USI-SUPSI<br/> Via la Santa 1, Lugano<br/> 6900, Switzerland
</p>
		</div>
	  </div>

      <center><p>&copy 2024 Gabriele Abbate </p></center>
	</div>
  </div>
</div>

<script src="/assets/javascript/bootstrap/jquery.min.js"></script>
<script src="/assets/javascript/bootstrap/bootstrap.bundle.min.js"></script>
<script src="/assets/javascript/agar_hide.js"></script>


  </body>

</html>
